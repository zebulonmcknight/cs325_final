{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from PIL import Image \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8aa53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class modeled after Pytorch dataset. Created to avoid 'class-per-folder' \n",
    "# organization that is required by standard pytorch datasets.\n",
    "\n",
    "class DogBreedDataset(Dataset):\n",
    "    # __init__ receives a pandas dataframe of form ('id')('breed'), a directory\n",
    "    # containing images, a dict of the form ('breed')(int label), and an optional\n",
    "    # transform pipeline. \n",
    "    def __init__(self, label_df, img_dir, labels, transform=None): \n",
    "        self.labels_df = label_df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = labels\n",
    "\n",
    "    # __len__ simply returns the number of samples in a dataset\n",
    "    def __len__(self): \n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    # __getitem__ takes in an index representing an entry in the dataset, it returns the img \n",
    "    # and proper class label associated with that index \n",
    "    def __getitem__(self, index): \n",
    "        row = self.labels_df.iloc[index]\n",
    "        path = os.path.join(self.img_dir, row[\"id\"]+ \".jpg\")\n",
    "        img = Image.open(path)\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transform: \n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.classes[row['breed']]\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in labels for each image \n",
    "labels = pd.read_csv(\"data/labels.csv\")\n",
    "breeds = labels['breed'].unique() \n",
    "\n",
    "# Dict mapping 'breed' to index, model will operate on integers and we will convert back to breeds \n",
    "classes = {b: i for i, b in enumerate(breeds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b28e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformers \n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)), # pytorch expects 224x224 sized images\n",
    "\n",
    "    # These transformations are standard for introducing more randomness into the training set. \n",
    "    # These transformations haven't been tested and perfected individually, but instead\n",
    "    # come as standard recommended parts of the pipeline for reducing overfitting and promoting\n",
    "    # generalization \n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.RandomResizedCrop(224, scale = (0.8, 1.0)),\n",
    "    transforms.RandomRotation(15), \n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3), \n",
    "    transforms.ToTensor(), \n",
    "\n",
    "    # Here we normalize RGB values to the mean and standard deviation of the ImageNet dataset, \n",
    "    # the set that our pretrained model was trained on. \n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # pytorch expects 224x224 sized images\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Here we normalize RGB values to the mean and standard deviation of the ImageNet dataset, \n",
    "    # the set that our pretrained model was trained on. \n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data into a dataset \n",
    "dataset = DogBreedDataset(labels, \"data/train\", classes, train_transforms)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e21ce305",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7170222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets stratified by class membership \n",
    "train_df, val_df = train_test_split(dataset.labels_df, \n",
    "                                    test_size=0.2, \n",
    "                                    stratify=dataset.labels_df['breed'])\n",
    "\n",
    "train_df.to_csv(\"train_split.csv\", index=False)\n",
    "val_df.to_csv(\"val_split.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958187f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split data into independent training and validation datasets \n",
    "train_dataset = DogBreedDataset(\n",
    "    label_df=train_df,\n",
    "    img_dir ='data/train',\n",
    "    labels = classes,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = DogBreedDataset(\n",
    "    label_df = val_df,\n",
    "    img_dir ='data/train',\n",
    "    labels = classes, \n",
    "    transform=val_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eed2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our dataloaders for training and validation\n",
    "train_loader = DataLoader(\n",
    "    # these variable primarily affect efficiency at fetching data\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # when running on windows num_worker must equal zero, there is a well known pytorch bug forcing this \n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f6afe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 0: Image shape = torch.Size([3, 224, 224]), Label = 65\n",
      "Item 1: Image shape = torch.Size([3, 224, 224]), Label = 113\n",
      "Item 2: Image shape = torch.Size([3, 224, 224]), Label = 25\n"
     ]
    }
   ],
   "source": [
    "# Test if the dataset itself is working\n",
    "for i in range(3):\n",
    "    try:\n",
    "        x, y = train_dataset[i]\n",
    "        print(f\"Item {i}: Image shape = {x.shape}, Label = {y}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=120, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# DEFINING MODEL SPECIFICATIONS\n",
    "\n",
    "device  = \"cpu\" # no gpu, so training on cpu \n",
    "\n",
    "# This line represents our model. For this implementation we are \n",
    "# using resnet18 with weights pretrained on ImageNetV1. This is an\n",
    "# industry standard Convolutional Neural Network designed for image\n",
    "# classification. \n",
    "cnn = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Here we freeze all but the last layer of the cnn, this lets us\n",
    "# start from a good place for image classification but lets us\n",
    "# design our own final layer for predictions. \n",
    "for param in cnn.parameters(): \n",
    "    param.requires_grad = False\n",
    "\n",
    "# Our final layer. This resembles a 2 layer MLP (multi layer perceptron)\n",
    "# That maps the hundreds or thousands of features present in the CNN \n",
    "# to the 120 outputs that we need to map to class probabilities. This layer is \n",
    "# the only layer we are training in our training loop. \n",
    "cnn.fc = nn.Sequential(\n",
    "    nn.Linear(cnn.fc.in_features, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.5), # Using dropout will hurt training performance, but decrease overfitting \n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "cnn=cnn.to(device)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # we'll be using CrossEntropy loss \n",
    "# This is our optimizer, ADAM is similar to an improved verison of stochastic \n",
    "# gradient descent designed specifically for deep learning\n",
    "optimizer = optim.Adam(cnn.fc.parameters(), lr=1e-4, weight_decay=1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP \n",
    "\n",
    "# Our training function takes in a reference to our model, our training and validation\n",
    "# loaders, a loss function, an optimizer, and a number of epochs to train for \n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs): \n",
    "    for epoch in range(num_epochs): \n",
    "        model.train()\n",
    "\n",
    "        # We will track loss and accuracy for each epoch \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # each epoch is broken into batches of 64 images\n",
    "        for inputs, labels in tqdm(train_loader): \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_loss} | Accuracy: {epoch_acc}\")\n",
    "\n",
    "        # Validation loop \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader: \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += (preds == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_corrects / len(val_loader.dataset)\n",
    "        print(f\"Validation Loss: {val_loss} | Accuracy: {val_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4c5b9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [02:33<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 4.445388658247616 | Accuracy: 0.08499449675920265\n",
      "Validation Loss: 4.108311125235336 | Accuracy: 0.24058679706601466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [02:41<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 4.061136771986184 | Accuracy: 0.15115568056744527\n",
      "Validation Loss: 3.597309883532722 | Accuracy: 0.34621026894865525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [03:03<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 3.643144564478733 | Accuracy: 0.21010150421915127\n",
      "Validation Loss: 3.1117800629809316 | Accuracy: 0.4444987775061125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [02:49<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 3.28300648063906 | Accuracy: 0.26232114467408585\n",
      "Validation Loss: 2.720641226873421 | Accuracy: 0.5080684596577018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [02:49<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 2.978823646467502 | Accuracy: 0.2979087684970038\n",
      "Validation Loss: 2.3912030819284302 | Accuracy: 0.5643031784841076\n"
     ]
    }
   ],
   "source": [
    "train(cnn, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5222fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = cnn.state_dict()\n",
    "\n",
    "torch.save(model_state, \"model_1.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
